# 微博「恐婚 · 恐育」数据分析 (Weibo Fear-of-Marriage & Childbirth Analysis)

## 项目背景 / Project Background

 本项目聚焦于分析微博上关于“恐婚”和“恐育”话题的讨论。随着生活成本上升、社会压力增大，许多年轻人表现出对婚姻和生育的恐惧或犹豫。本研究通过对相关微博内容的挖掘，试图揭示年轻人产生“恐婚恐育”心理的主要原因、讨论的焦点，以及公众在这一话题上的情感倾向和立场态度。项目旨在帮助更好地理解当代年轻人在婚姻与生育问题上的观念，为社会学研究和相关决策提供数据支持。

This project focuses on analyzing discussions on Weibo about the fear of marriage (“恐婚”) and fear of childbearing (“恐育”). As the cost of living rises and social pressures increase, many young people are expressing anxiety or hesitation toward marriage and parenthood. Through mining relevant Weibo posts, this research seeks to uncover the primary reasons behind young people’s fear of marriage/childbirth, the key points of discussion, and the public’s sentiment and stance on this topic. The goal is to better understand contemporary youths’ perspectives on marriage and having children, providing data-driven insights for sociological research and informed decision-making.

## 数据来源 / Data Source

 数据来源于微博公开内容。本项目利用网络爬虫收集了微博上带有话题标签“#年轻人恐婚恐育的真正原因是什么#”的帖子及评论。爬虫通过微博的搜索接口按关键词抓取相关数据（包括微博正文、发布时间、发布者等公开信息）。需要注意的是，数据均来源于公开网页，项目严格遵守数据隐私规范，仅对公开讨论内容进行分析。

The dataset comes from publicly available Weibo content. We used a web crawler to collect posts and comments containing the hashtag “#年轻人恐婚恐育的真正原因是什么#” (“What is the real reason young people fear marriage and childbearing?”). The crawler leveraged Weibo’s search interface to fetch relevant data by keyword, including the post text, timestamps, user IDs, and other public information. All data was obtained from public web pages, and the project adheres to privacy guidelines by analyzing only publicly available discussion content.

## 分析流程 / Analysis Process
数据分析流程分为以下几个步骤：

1. **数据清洗 (Data Cleaning)：** 对原始爬取的微博数据进行清洗和预处理。首先去除重复帖子和过短内容，然后清理文本中的URL链接、表情符号、“@用户名”、标点符号等噪音字符。接着，将文本统一转为小写（如适用）并删除多余空白。随后进行中文分词并过滤停用词，同时将常见同义词统一（例如，将“结婚”统一为“婚姻”，“不婚”统一为“恐婚”等）。清洗后的数据保存为`weibo_cleaned.csv`，包含干净文本和对应的分词结果。
2. **主题模型 (Topic Modeling)：** 利用清洗后的文本进行主题挖掘。本项目采用了两种主题建模方法：**LDA**和**BERTopic**。首先使用LDA (Latent Dirichlet Allocation) 进行基准主题提取，初始主题数设定为k=18，并通过计算一致性(coherence)评估不同主题数量的模型，寻找最佳主题数。LDA模型提取了18个主题，每个主题给出了若干高频关键词。例如，某些主题围绕“婚姻、成本、压力、独生子女”等关键词，反映了经济压力与独生子女政策相关的讨论；另一些主题包含“家暴、出轨、法律援助”等词，表明一部分用户关注婚姻中的负面事件（家庭暴力、出轨）及法律咨询；还有主题出现了“张歆艺、袁弘、208”等关键词，暗示一些讨论涉及特定事件或人物（如明星谈论生育观）。其次，使用BERTopic模型对微博文本进行聚类分析。BERTopic利用预训练的多语句BERT嵌入和基于TF-IDF的词向量，对语义相似的帖子进行聚类，并自动合并小主题。BERTopic的结果提供了主题数量、每个主题的代表词和包含的帖子数等信息（结果导出为`bertopic_topic_info.csv`）。总体而言，主题模型帮助我识别了微博讨论中隐含的主要议题，例如：经济压力与成本、高负面案例导致的恐婚心理、社会环境与个人价值观、以及围绕热点人物/事件的讨论等。
3. **情感分析与立场识别 (Sentiment & Stance Analysis)：** 在主题挖掘的基础上，本项目进一步对微博内容的情感倾向和观点立场进行了分析。情感分析包括**二分类情感**（正面/负面）和**多类别情绪**识别。我使用了开源的中文预训练模型对每条微博文本进行情感判别，输出其为正面或负面的概率。例如，统计结果显示大部分帖子的情绪偏消极，整体**负面情感概率**显著高于正面概率（反映出讨论“恐婚恐育”时普遍带有担忧、焦虑等负面情绪）。同时，我采用一个支持8种细分情绪类别的模型，对每条帖子进行情绪强度分析，包括“开心、愤怒、悲伤、平淡、关切（担忧）、疑问、惊奇、厌恶”等。结果表明，在所有情绪中，“关切（担忧）”和“平淡”这两类情绪的平均占比最高，而正向情绪如“开心”的占比很低；此外，“愤怒”和“悲伤”等负向情绪也有相当占比，体现了一些用户在讨论该话题时的愤懑或无奈。除了情感倾向，我还通过**立场分析**来判断微博作者对“恐婚恐育”话题所持的态度。借助自然语言推理(NLI)模型，我将每条帖子与预设的命题进行匹配，判断其是**支持**“恐婚/恐育”、**反对**“恐婚/恐育”还是保持**中立**。整体来看，多数用户的态度趋于中立理性，他们往往只是陈述原因并未明确表态支持或反对；同时，相当一部分用户表达了对恐婚恐育的“支持/认同”态度（即自身也有恐婚恐育倾向，或者理解这种恐惧），而明确“反对/不认同”该现象的态度占比最小。我进一步计算了不同情绪与立场之间的相关性，并绘制了**情绪-立场相关热力图**【49†】：结果显示“愤怒”情绪与反对立场呈较强正相关，而“悲伤”“关切”等情绪更多出现在支持恐婚恐育的言论中；反之，表达开心正向情绪的帖子往往与反对恐婚恐育的立场略有相关，这可能意味着认为不应恐婚的人群中有少许积极情绪者。上述情感和立场分析结果综合反映了微博讨论中人们的心理倾向：一方面，整体氛围偏向担忧和消极；另一方面，大部分人对此话题持理解态度，只有少数人在积极反驳这种观念。
4. **网络分析 (Network Analysis)：** 最后，我对讨论中的关键词共现关系和用户互动关系进行了网络分析。首先构建了**关键词共现网络**：将微博内容分词后，每出现一次词语共同出现（co-occurrence），在对应两个词之间连一条边，并累积权重。本项目仅保留共同出现次数不少于5次的关键词对，从而得到一个较为稠密的语义网络。在该网络中，有191个节点（关键词）和2896条边；我采用Louvain算法对网络进行社群划分，结果显示网络中存在若干**语义社群**。例如，一个社群围绕“婚姻、孩子、经济、压力、成本、独生子女”等词，体现了**经济压力和现实成本**这一主题；另一个社群聚合了“家暴、出轨、法律、律师、视频、报警、援助12348”等词，代表**负面婚姻事件及法律咨询**相关讨论；还有社群以“张歆艺、袁弘、恐婚、生育观、208”等为核心，指向**特定人物事件**（知名艺人发表不婚不育观点引发的话题）；以及一些社群包含“选择、自由、幸福、生活态度”等词语，涉及**个人选择与价值观**层面的讨论。在关键词共现图中，节点大小反映词语出现的频繁程度或连接度，连线粗细反映词语在帖子中共同出现的频率。通过交互式网络图，我可以直观观察到高频关键词之间的连接——例如，“婚姻”与“孩子”“成本”“压力”紧密相连，表示人们常在同一语境下提及结婚和生育所伴随的经济与压力问题；又如，“家暴”与“法律援助”连接，表示有不少帖子讨论因家庭暴力导致对婚姻的恐惧，并附有法律求助的信息。其次，如果数据中包含转发关系，我也构建**用户互动网络**：节点表示用户，定向边表示用户之间的转发/引用关系，边权重为互动次数。由于本话题的互动数据有限，用户网络较稀疏（若无足够的`uid`和`retweeted_uid`字段则未绘制此网络）。总体而言，网络分析从宏观上揭示了讨论的结构：不同议题板块之间的关联和聚集情况，以及（潜在的）核心传播用户。

The analysis workflow can be summarized in the following steps:

1. **Data Cleaning:** We first cleaned and preprocessed the raw Weibo data. This involved removing duplicate posts and very short texts, then stripping noise from the content (such as URLs, emojis, “@user” mentions, and punctuation). The text was normalized (e.g., lowercased as needed), and excessive whitespace was trimmed. We then performed Chinese word segmentation (tokenization) and filtered out stopwords, also normalizing certain synonyms (for example, “结婚” (to marry) was unified as “婚姻” (marriage), “不婚” (no-marriage) was mapped to “恐婚” (fear of marriage), etc.). The cleaned results, including the processed text and token lists, were saved to `weibo_cleaned.csv`.
2. **Topic Modeling:** Using the cleaned text, we extracted latent topics from the discussions. Two approaches were applied: **LDA** and **BERTopic**. We first ran an LDA (Latent Dirichlet Allocation) model as a baseline, setting the number of topics to k=18 initially. We evaluated model coherence scores for different values of k to find an optimal topic count. The LDA model with k=18 produced 18 distinct topics, each characterized by a set of top keywords. For instance, some topics centered on terms like “marriage, cost, pressure, only-child,” reflecting discussions about **financial burdens and the one-child generation**; other topics included words such as “domestic violence, infidelity, legal aid,” indicating a subset of users focused on **negative marital experiences and legal counseling**; there was also a topic containing “Zhang Xinyi, Yuan Hong, 208,” suggesting that **specific public figures or events** (e.g. celebrities expressing views on not having children) were part of the conversation. Next, we applied the BERTopic model to cluster posts based on semantic similarity. BERTopic uses a pretrained multilingual sentence transformer to embed each post, then performs clustering with a class-based TF-IDF mechanism, automatically merging small topics. The BERTopic results (exported as `bertopic_topic_info.csv`) provide the number of topics found, representative keywords for each topic, and the number of posts per topic. In summary, topic modeling helped identify the main themes in the Weibo discussion, such as: economic pressure and cost of living, fear of marriage due to negative cases, societal environment vs personal values, and discussions sparked by trending people/events.
3. **Sentiment & Stance Analysis:** Building on the topic modeling, we analyzed the sentiment polarity and stance of the posts. For sentiment, we performed both **binary sentiment classification** (positive vs. negative tone) and **multi-class emotion detection**. Using a pre-trained Chinese sentiment model, we obtained a positive/negative probability for each post. The overall distribution showed that the vast majority of posts had a **negative sentiment tendency**, with negative sentiment probabilities significantly higher than positive – reflecting that discussions around “fear of marriage/childbirth” are largely tinged with worry or anxiety. Additionally, we employed a model capable of recognizing 8 emotional categories in text (joy, anger, sadness, neutral/calm, concern, doubt, surprise, disgust). The analysis revealed that among these emotions, **concern (worry)** and **calm (neutral)** were the most dominant on average, while positive emotion like **joy** was very low; notably, **anger** and **sadness** also appeared with considerable frequency, indicating that some users express frustration or sorrow in this context. Beyond sentiment, we performed a **stance analysis** to infer each author’s attitude toward the issue of “fearing marriage/childbirth.” Using a zero-shot NLI (Natural Language Inference) approach, we compared each post against two hypotheses – one indicating the post **supports** the fear-of-marriage/childbirth perspective, and one indicating it **opposes** it – as well as a neutral option. Overall, we found that the majority of users were **neutral or factual** in tone, often simply stating reasons without explicitly supporting or opposing the idea. A substantial portion of users showed a **supportive** stance toward the fear (either sharing their own fear or empathizing with it), whereas a smaller minority took an **opposing** stance (i.e. arguing against the fear-of-marriage mindset). We also calculated the correlation between emotional tone and stance, visualizing it in an **Emotion-Stance correlation heatmap**【49†】. The results suggest that **anger correlates strongly with an opposing stance** (users who were angry tended to criticize or reject the fear-of-marriage phenomenon), while emotions like **sadness** and **concern** were more associated with a supportive stance (those who fear marriage often expressed worry or sorrow). Conversely, posts exhibiting **joy** or positive emotion were slightly more likely to come from users opposing the fear-of-marriage idea. These sentiment and stance analyses together paint a picture of the discussion’s tone: on one hand, the overall mood is predominantly worried and negative; on the other hand, most people are understanding of the fear (or share it themselves), with relatively few actively arguing against it.
4. **Network Analysis:** Finally, we examined the network of keywords and user interactions within the discussion. We built a **keyword co-occurrence network** where each node is a significant word, and an edge between two words indicates they co-occurred in the same post (with at least 5 co-occurrences required to be included). This produced a fairly dense semantic network – the graph contains 191 nodes (keywords) and 2896 edges. We applied the Louvain community detection algorithm to identify clusters of related terms. The network revealed several distinct **semantic communities**. For example, one cluster of nodes – including “marriage, children, economy, pressure, cost, only-child” – represents the theme of **economic pressures and practical costs** in starting a family. Another cluster connects terms like “domestic violence, cheating, law, lawyer, video, police, help line 12348,” corresponding to discussions about **negative marital incidents and seeking legal help**. A further cluster centered on names like “Zhang Xinyi, Yuan Hong, fear of marriage, views on having children, 208” points to **specific public figures/events** (such as a well-known celebrity couple who publicly discussed not wanting children). There are also clusters featuring words like “choice, freedom, happiness, life” which relate to **personal choice and values**. In the co-occurrence graph, the node size reflects a word’s frequency/degree, and edge thickness reflects how often the two words appeared together. The interactive visualization allows us to see high-frequency terms and their connections – for instance, “marriage” is tightly connected with “children,” “cost,” and “pressure,” indicating that posts frequently mention the financial and societal pressures of marriage and childbearing in one breath. Similarly, “domestic violence” is linked to “legal aid,” showing that quite a few posts discuss fears stemming from abusive relationships and provide information on seeking help. In addition to the keyword network, we also considered a **user interaction network** based on repost/retweet data. In this directed graph, nodes represent users and an arrow from user A to user B indicates A reposted B’s content (with edge weight as the number of interactions). For this particular topic, the user interaction data was sparse; if present at all, the user network had relatively few active nodes and edges (and if the dataset lacked the `uid` and `retweeted_uid` fields, this network was skipped). In general, the network analysis provides a macro-level view of the discussion structure: how different sub-topics are interlinked or clustered, and (potentially) which users were central in spreading the conversation.

## 项目结构 / Project Structure

本项目的代码和目录结构清晰地反映了分析流程，各Python模块的功能如下：

* **`utils/spider.py`** – 数据爬取脚本。用于通过微博搜索接口抓取含指定话题标签的微博数据。需要预先设置Cookies等参数来模拟登录环境。该脚本将爬取的原始微博数据保存为CSV文件（例如`年轻人恐婚恐育的真正原因是什么.csv`），包含微博内容、发布时间、用户ID等。
* **`utils/clean.py`** – 数据清洗与预处理。读取爬虫所得的原始CSV（通过调整`DATA_PATH`指定文件路径），执行重复内容去除、短文本过滤、文本正则清洗（去除链接、表情、@等）、时间格式解析等操作，然后对微博文本进行中文分词和停用词过滤，将同义词标准化。清洗结果包括新增的`clean_text`列（清理后的文本）和`tokens`列（分词列表），最终保存至`outputs/weibo_cleaned.csv` 供后续分析使用。
* **`utils/topic_modeling.py`** – 主题模型训练与可视化。读取清洗后的数据，利用BERTopic和LDA两种方法提取主题。首先加载预训练的多语言句向量模型（用于BERTopic），配置中文分词和停用词，训练BERTopic模型并自动合并相近小主题。BERTopic结果保存模型文件并导出主题信息CSV，以及生成交互式主题可视化HTML（`bertopic_topics.html`）。接着训练LDA模型（默认主题数k=18），输出每个主题的关键词列表到文本文件。计算模型一致性评分以辅助选择最佳主题数，并绘制不同主题数下的一致性曲线图`lda_k_selection.png`。最后使用PyLDAvis库生成LDA主题的交互式可视化HTML（`lda_vis_18.html`），可展示主题间距离和词频分布。
* **`utils/sentiment_analysis.py`** – 情感与立场分析。载入清洗后的微博文本，分别进行情感倾向分析和立场判断。首先采用预训练模型对每条微博进行**正负面情感**分析，将结果（正面概率pos\_prob和负面概率neg\_prob）保存为`outputs/weibo_sent_binary.csv`。然后使用预训练的8类情绪分类模型，对每条微博输出属于“平淡、关切、开心、愤怒、悲伤、疑问、惊奇、厌恶”各情绪的概率分布，结果保存为`weibo_sent_emotion.csv`。接下来进行**立场判定**：利用中文NLI模型，将每条微博分别与“这条微博支持恐婚或恐育”“这条微博反对恐婚或恐育”两个假设句子进行匹配，计算蕴含(entail)概率、中立概率和矛盾(反对)概率。简化起见，将针对支持假设的“蕴含”概率视为该帖支持立场的分数，针对反对假设的“蕴含”概率视为反对立场分数，中立则取两个假设下模型判断为中立的平均。最终保存立场结果到`weibo_stance.csv`。
* **`utils/analyze_results_sentiment.py`** – 情感与立场结果分析及可视化。该脚本读取上一步生成的三个CSV文件（binary情感、emotion情绪、stance立场），计算总体统计并生成图表。例如，绘制情感概率分布的直方图、各类情绪平均概率的条形图、立场分布的饼图，以及情绪与立场相关性的热力图。每个图表既保存为静态PNG文件（便于在README或报告中查看），又生成交互式图形并整合在单页仪表板HTML（`sentiment_dashboard.html`）中。
* **`utils/network_analysis.py`** – 网络关系分析与可视化。载入清洗后的数据，构建关键词共现网络和用户转发网络。对于**关键词共现网络**，统计所有帖子中词语两两共现的次数（至少出现5次以上才连边），生成无向图并用Louvain算法检测社区，将社区类别作为节点属性用于配色。然后利用PyVis库将网络图保存为交互HTML（`co_network.html`），节点大小根据度数(degree)显示，共现次数作为边的权重并可在悬停时提示。对于**用户互动网络**，如果数据包含用户ID和被转发用户ID，则依据转发关系构建有向图，节点为用户，边从发帖者指向被转发者，边权重为转发次数。同样应用社区划分用于节点分组，并将图导出为`user_network.html`。最后输出网络统计信息如节点数、边数、密度等到文本文件`network_stats.txt`。如果原始数据没有转发信息，则跳过用户网络部分。

The project’s code and directory structure mirror the analysis steps. Each Python module has the following role:

* **`utils/spider.py`** – **Data Crawler**. This script is responsible for fetching Weibo data via the search API using the specified hashtag or query. It requires a valid user session (cookies) to access search results. The spider iterates through search result pages (and can be configured with date ranges or sort options) to collect posts. It extracts fields like content, timestamps, user IDs, etc., and saves the raw data to a CSV file (for example, `年轻人恐婚恐育的真正原因是什么.csv`).
* **`utils/clean.py`** – **Data Cleaning & Preprocessing**. This module reads the raw CSV from the crawler (adjust `DATA_PATH` to the file location) and performs cleaning operations: removing duplicate entries and very short posts, cleaning text by removing URLs, emojis, “//@...” repost headers, and punctuation. It then parses the timestamp column (if present) into datetime format. After that, it tokenizes the Chinese text into words, filters out stopwords (with a provided stopword list), and normalizes synonyms (e.g., “不婚”→“恐婚” for consistency). The output is a dataframe with a new `clean_text` column (the cleaned text) and a `tokens` column (list of tokens), which is then saved to `outputs/weibo_cleaned.csv` for use in later steps.
* **`utils/topic_modeling.py`** – **Topic Modeling & Visualization**. This script trains the BERTopic and LDA models on the cleaned data. It starts by loading a pretrained multilingual sentence embedding model and setting up a custom tokenizer (using Jieba for Chinese) along with a stopword list. The script then fits a BERTopic model to cluster the posts and automatically reduce/merge similar small topics. It saves the BERTopic model to disk and outputs a CSV (`bertopic_topic_info.csv`) with topic numbers, sizes, and representative words. It also generates an interactive topic visualization HTML (`bertopic_topics.html`) showing topic distances and prevalence. Next, the script trains an LDA model (with number of topics k=18 by default). It writes out the top keywords for each LDA topic to a text file, and computes the model’s coherence value. The script optionally evaluates different k values (e.g., 8 to 30) to find the best coherence, and plots the coherence vs. topic count curve in `lda_k_selection.png`. Finally, it uses PyLDAvis to create an interactive visualization of the LDA topics (`lda_vis_18.html`), which allows exploring the inter-topic distance map and term frequency distributions.
* **`utils/sentiment_analysis.py`** – **Sentiment and Stance Inference**. This module takes the cleaned tweets and performs sentiment classification and stance detection. It first uses a pretrained model (`uer/roberta-base-finetuned-jd-binary-chinese`) to predict binary sentiment for each post (positive vs. negative). The script records the positive probability and negative probability for each entry and saves these results to `outputs/weibo_sent_binary.csv`. Then it uses an 8-class emotion classifier (`Johnson8187/Chinese-Emotion`) to get probabilities for each of eight emotion categories (“neutral, concern, joy, anger, sadness, doubt, surprise, disgust”) for every post. These probabilities are saved to `weibo_sent_emotion.csv`. After sentiment, the script performs **stance detection** using a Chinese NLI model (`IDEA-CCNL/Erlangshen-Roberta-110M-NLI`). It defines two hypothesis sentences – one that the post **supports** the fear-of-marriage/childbearing topic and one that it **opposes** it – and checks each post against both hypotheses. By examining the entailment vs. contradiction probabilities from the NLI model, the script assigns a stance score for “support,” “neutral,” and “oppose” for each post. These are simplified such that the entailment probability with the support hypothesis is taken as the support score, the entailment probability with the oppose hypothesis as the oppose score, and the average neutral probability from both as the neutrality score. The stance results are saved to `weibo_stance.csv`.
* **`utils/analyze_results_sentiment.py`** – **Sentiment & Stance Analysis Visualization**. This script reads the three CSV files produced by the sentiment analysis step (binary sentiment, emotion, stance). It computes overall statistics and generates visualizations. For example, it prints the total number of samples and average positive/negative sentiment probability. It then creates a sentiment distribution histogram (overlapping positive and negative probability histograms), a bar chart of average probabilities for each emotion category, a pie chart showing the average distribution of stance (support/neutral/oppose), and a heatmap of the correlation between each emotion and stance. Each plot is saved as a static PNG image (for quick preview in documentation) and also combined into an interactive Plotly dashboard HTML (`sentiment_dashboard.html`) for exploration. The interactive dashboard allows zooming and hovering to see exact values, and it begins with a title and instructions in Chinese.
* **`utils/network_analysis.py`** – **Network Construction & Visualization**. This module reads the cleaned data and constructs the keyword co-occurrence network and the user repost network. For the **keyword co-occurrence network**, it iterates through each post’s token list and counts every pair of words that appear together. Pairs with frequency below 5 are ignored to reduce noise. An undirected weighted graph is built (words as nodes, co-occurrence counts as edge weights). The script then applies the Louvain community detection to find clusters of related words, storing the community assignment as a node attribute for coloring. A PyVis interactive graph is generated and saved to `co_network.html`, with node size representing the word’s degree (number of connections) and edges configured to show weight on hover. For the **user interaction network**, if the dataset contains user IDs and retweeted user IDs, the script creates a directed graph where each edge from user A to user B indicates A reposted B’s content (with weight as the count of reposts). It runs community detection on the undirected version of this graph to group users, then generates an interactive network HTML (`user_network.html`) with nodes sized by their degree and arrows on edges to indicate direction. Finally, the script writes out some network metrics (number of nodes, edges, graph density for both networks) to `network_stats.txt`. If the required columns for user network are missing or if there were no retweet interactions, it skips the user network part (logging a warning).

## 可视化结果 / Visualization Results

项目的主要分析结果均已通过可视化进行呈现，可在`docs/outputs`目录下查看交互式的HTML文件或静态图片，**index.html为汇总版的可视化结果**：

* **BERTopic主题可视化：** `docs/outputs/bertopic_topics.html` 展示了BERTopic模型得到的主题分布图。每个气泡代表一个主题，大小对应主题包含的微博数量，气泡之间的距离反映主题间的相似度。通过悬停可查看各主题的代表关键词和权重。该图有助于直观了解BERTopic提取的主题及它们的关联。
* **LDA主题可视化：** `docs/outputs/lda_vis_18.html` 是LDA模型的交互式可视化（由PyLDAvis生成）。左侧是“主题间距离图 (Intertopic Distance Map)”【15†】——每个圆点为一个主题，面积表示主题占比，图中主题的相对距离表示它们在语料中的区别度；右侧则显示选中主题的详细信息，包括该主题的Top词语及其频率分布情况。用户可以通过点击不同主题来观察其关键词组成和与其他主题的关系。
* **情感与立场仪表板：** `docs/outputs/sentiment_dashboard.html` 汇总了情感分析和立场分析的互动式图表。在页面中，可以找到“情感分布直方图” （展示总体正负面情感倾向的分布情况）、“各情绪类别平均概率条形图”（比较八种情绪在帖子中的平均强度）、“立场分布饼图”（展示支持/中立/反对三类立场的总体占比）以及“情绪与立场相关性热力图”（显示不同情绪强度与持支持/中立/反对立场的相关程度）。通过这些可视化，读者可以方便地观察微博讨论的情感基调和态度倾向，例如负面情感明显多于正面、用户整体上以中立或支持态度居多等。所有图表均支持鼠标悬停查看具体数值，缩放拖拽等交互操作。静态的PNG图片（如`sentiment_distribution.png`, `emotion_distribution.png`, `stance_distribution.png`, `emotion_stance_correlation.png`）也提供用于快速预览。
* **关键词共现网络：** `docs/outputs/co_network.html` 是关键词共现关系的交互网络图。打开后，可以看到大量节点（词语）通过线条相连构成的网络。其中同一颜色的节点群表示由Louvain算法识别出的语义社区，节点大小表示词的重要性（度数或出现频率），连线粗细表示词语共同出现的频繁程度。用户可以拖拽节点查看局部结构，或使用左下角的控制按钮放大、缩小网络图。此外，点击右上方的“physics”开关可以暂停/恢复力导向布局，以便固定网络布局进行观察。这个共现网络可以帮助理解讨论中哪些概念是紧密关联的，例如当你看到“婚姻”“孩子”“经济”“压力”聚在一起且彼此相连，就能体会到经济压力是导致恐婚的重要议题之一。
* **用户互动网络：** 如果有生成，此网络图文件为 `docs/outputs/user_network.html`，展示用户之间的转发关系网络。节点代表用户，箭头指向被转发的对象，节点大小通常体现该用户的活跃度或被关注度（例如度数）。通过该图可以发现可能的话题传播核心用户或社区。然而在本项目的数据中，用户互动关系较弱，所以此图如果存在也是稀疏网络，仅供参考。

The key findings of the project are presented through various visualizations. These can be accessed in the `docs/outputs` directory, either as interactive HTML files or as static images for quick viewing:

* **BERTopic Topic Visualization:** `docs/outputs/bertopic_topics.html` contains an interactive visualization of the topics derived from BERTopic. Each bubble represents a topic, with its size proportional to the number of posts in that topic. The distance between bubbles indicates how similar or distinct the topics are. Hovering over a bubble will show the top keywords that define that topic and their relative importance. This allows an at-a-glance understanding of the themes BERTopic found and how they relate to each other.
* **LDA Topic Visualization:** `docs/outputs/lda_vis_18.html` is the interactive visualization for the LDA model (generated by PyLDAvis). On the left, it shows the **Intertopic Distance Map**【15†】 – each circle is an LDA topic, with area corresponding to the topic’s prevalence, and the distance between circles reflecting how different the topics are. On the right, it displays the details of the selected topic, including its top terms and the term frequency within that topic versus the corpus. You can click on any topic circle to see the word distribution for that topic and explore how topics overlap or separate in terms of content.
* **Sentiment & Stance Dashboard:** `docs/outputs/sentiment_dashboard.html` provides an interactive dashboard summarizing sentiment and stance analysis results. On this page you will find a *Sentiment Distribution Histogram* (showing the distribution of positive vs. negative sentiment scores across posts), a *Mean Emotion Probability Bar Chart* (comparing the average intensities of the eight emotion categories), a *Stance Distribution Pie Chart* (depicting the overall proportion of support/neutral/oppose stance probabilities), and an *Emotion-Stance Correlation Heatmap* (illustrating how each emotion correlates with holding a supportive, neutral, or opposing stance). These visuals enable the reader to quickly grasp the emotional tone and attitude trends in the discussion – for example, that negative sentiment overwhelmingly exceeds positive, or that the majority stance is neutral/supportive rather than outright opposing. All charts are interactive, allowing you to hover to see exact values and use zoom/pan controls. Static PNG images (such as `sentiment_distribution.png`, `emotion_distribution.png`, `stance_distribution.png`, `emotion_stance_correlation.png`) are also provided for a quick preview or inclusion in reports.
* **Keyword Co-occurrence Network:** `docs/outputs/co_network.html` is an interactive network graph of keyword co-occurrences. Upon opening, you will see numerous nodes (words) connected by links, forming a network. Nodes with the same color belong to the same community as determined by the Louvain algorithm, indicating they form a cluster of related terms. The node size represents its importance (degree or frequency of occurrence), and the thickness of the link represents how often the two words co-occurred. You can drag nodes to rearrange and inspect local structures, or use the green buttons in the lower-left to zoom and shift the view. There is also a physics control toggle (upper-right) to pause the force-directed layout for easier examination of the network once it stabilizes. This co-occurrence network helps reveal which concepts are closely linked in discussions. For instance, seeing “marriage,” “children,” “economy,” and “pressure” clustered and interconnected confirms that financial pressure is a key theme intertwined with fear of marriage.
* **User Interaction Network:** If generated, the user network visualization is available as `docs/outputs/user_network.html`, illustrating the retweet/repost relationships among users. Each node represents a user, and an arrow from one node to another indicates a repost from the former to the latter; node sizes may reflect user activity or reach (e.g., degree). This graph can highlight any central influencers or communities in the spread of the topic. However, in our dataset the user interaction was minimal, so if this graph is present, it will be quite sparse and should be interpreted with caution (many users discussing the topic did so independently rather than via resharing).

*(To view the interactive HTML visualizations locally, you may need to serve the `docs` folder with a simple HTTP server due to browser security restrictions on local files. For example, run `python -m http.server 8000` in the project directory, and then navigate to `http://localhost:8000/docs/index.html` for an integrated view of all results.)*

## 使用说明 / Usage Instructions

若要在本地运行本项目或复现分析，请确保具备合适的Python环境（建议Python 3.9+）并安装所有依赖库。主要依赖包括：`pandas`、`numpy`、`jieba`、`tqdm`、`matplotlib`、`seaborn`、`plotly`、`networkx`、`python-louvain`、`pyvis==0.3.2`、`gensim`、`pyLDAvis`、以及`torch`和`transformers`库（用于深度学习模型）。具体版本要求可参考`requirements.txt`。另外，运行情感和立场分析部分需要预先下载HuggingFace的模型，如上述使用的**UER-RoBERTa情感分析模型**和**IDEA-CCNL RoBERTa NLI模型**等，这些会在代码首次执行时自动下载（需要确保联网环境）。

使用流程如下：先运行 `utils/spider.py` 完成数据爬取（根据需要修改搜索关键词或时间范围，注意需要有效的Cookies）；然后运行 `utils/clean.py` 对原始数据进行清洗预处理。接着，运行 `utils/topic_modeling.py` 训练主题模型并生成可视化结果；运行完成后，可以在控制台查看LDA模型的一致性得分和最佳主题数提示，并在`docs/outputs/`目录找到主题模型的输出文件。之后，运行 `utils/sentiment_analysis.py` 获取情感和立场结果（该步骤可能稍耗时，尤其在CPU环境下处理上千条文本需要几十分钟，因为涉及深度学习模型推理）；再运行 `utils/analyze_results_sentiment.py` 生成情感立场的图表和仪表板HTML。最后，运行 `utils/network_analysis.py` 生成关键词共现和用户网络的HTML可视化及统计信息。全部步骤完成后，`docs/outputs` 文件夹下应产生所有结果文件。您可以直接打开 `docs/index.html` 在浏览器中查看整合的分析报告页面，该页面包含导航栏，可切换浏览BERTopic主题、LDA主题、情感与立场分析仪表板、以及网络分析图等模块。
<pre lang="md"> ```bash # 安装依赖 pip install -r requirements.txt # 执行分析各模块 python utils/clean.py python utils/topic_modeling.py python utils/sentiment_analysis.py python utils/analyze_results_sentiment.py python utils/network_analysis.py ``` </pre>

To run this project or reproduce the analysis, ensure you have a proper Python environment (Python 3.9+ recommended) and all required libraries installed. Key dependencies include: `pandas`, `numpy`, `jieba`, `tqdm`, `matplotlib`, `seaborn`, `plotly`, `networkx`, `python-louvain`, `pyvis==0.3.2`, `gensim`, `pyLDAvis`, as well as `torch` and `transformers` for the deep learning models. Please refer to `requirements.txt` for specific version requirements. Additionally, the sentiment and stance analysis steps rely on pretrained models from HuggingFace (such as the UER-RoBERTa sentiment model and the IDEA-CCNL RoBERTa NLI model). These will be downloaded automatically when the code is first run, so an internet connection is needed for that step.

The typical usage workflow is: run `utils/spider.py` first to crawl the data (modify the query hashtag or time range in the script as needed, and ensure you have valid Weibo cookies/session set up). After crawling, run `utils/clean.py` to preprocess and clean the raw data. Next, execute `utils/topic_modeling.py` to train the topic models and produce visualizations. Once this finishes, you can check the console output for the LDA coherence scores and the recommended optimal number of topics, and find the output files in `docs/outputs/` (including topic word lists and interactive HTML). Then run `utils/sentiment_analysis.py` to perform sentiment and stance inference on the posts (note: this step can be time-consuming, especially on CPU, as it processes each post with transformer models – processing a few thousand posts may take tens of minutes). After that, run `utils/analyze_results_sentiment.py` to generate charts and the dashboard HTML for sentiment and stance results. Finally, run `utils/network_analysis.py` to create the keyword co-occurrence and user interaction network visualizations and compute network metrics.

After completing all steps, you should find all result files in the `docs/outputs` folder. For a convenient view of the analysis, open `docs/index.html` in a web browser – it serves as a unified report page with a navigation menu, allowing you to switch between the BERTopic results, LDA visualization, sentiment & stance dashboard, and network graphs.
<pre lang="md"> ```bash # Install dependencies pip install -r requirements.txt # Run each analysis module python utils/clean.py python utils/topic_modeling.py python utils/sentiment_analysis.py python utils/analyze_results_sentiment.py python utils/network_analysis.py ``` </pre>







